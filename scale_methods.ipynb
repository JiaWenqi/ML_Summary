{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"TestExample\").enableHiveSupport()\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 归一化\n",
    "将特征属性缩放到一个指定范围，也即将属性缩放到一个指定的最大和最小值之间。\n",
    "- 这里有两种方法，首先介绍的是MaxAbsScaler，这种方法是计算dataframe中每列的最大值，然后将每列中的每个元素值与每相对应的每列的最大值相除，然后得到每列的元素值区间在[-1, 1]之间的dataframe。这种方法不会使得数据偏移或者中心化（shift/center the data），因此不会破坏数据的稀疏性（thus does not destroy any sparsity）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|      features|\n",
      "+---+--------------+\n",
      "|  0|[1.0,0.1,-8.0]|\n",
      "|  1|[2.0,1.0,-4.0]|\n",
      "|  2|[4.0,10.0,8.0]|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -8.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.0, -4.0]),),\n",
    "    (2, Vectors.dense([4.0, 10.0, 8.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "dataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute summary statistics and generate MaxAbsScalerModel\n",
    "scalerModel = scaler.fit(dataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|      features|  scaledFeatures|\n",
      "+--------------+----------------+\n",
      "|[1.0,0.1,-8.0]|[0.25,0.01,-1.0]|\n",
      "|[2.0,1.0,-4.0]|  [0.5,0.1,-0.5]|\n",
      "|[4.0,10.0,8.0]|   [1.0,1.0,1.0]|\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rescale each feature to range [-1, 1].\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "scaledData.select(\"features\", \"scaledFeatures\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 另外一种方法是MinMaxScaler，这种方法是将dataframe的每列数据的每个元素缩放到[0,1]的数值区间内，其每个元素的计算公式是\\begin{equation}\n",
    "  Rescaled(e_i) = \\frac{e_i - E_{min}}{E_{max} - E_{min}} * (max - min) + min\n",
    "\\end{equation}\n",
    "其中$E_{max}$是每列的最大值，$E_{min}$是每列的最小值,并且$max=1，min=0$.当$E_{max} == E_{min}，Rescaled(e_i) = 0.5 * (max + min)$,也即某列的所有元素都相等时，归一化的结果是每个元素的值等于0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|          features|\n",
      "+---+------------------+\n",
      "|  0|[1.0,0.1,-1.0,3.0]|\n",
      "|  1| [2.0,1.1,1.0,3.0]|\n",
      "|  2|[3.0,10.1,3.0,3.0]|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -1.0, 3.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0, 3.0]),),\n",
    "    (2, Vectors.dense([3.0, 10.1, 3.0, 3.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "dataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(dataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled to range: [0.000000, 1.000000]\n",
      "+------------------+-----------------+\n",
      "|          features|   scaledFeatures|\n",
      "+------------------+-----------------+\n",
      "|[1.0,0.1,-1.0,3.0]|[0.0,0.0,0.0,0.5]|\n",
      "| [2.0,1.1,1.0,3.0]|[0.5,0.1,0.5,0.5]|\n",
      "|[3.0,10.1,3.0,3.0]|[1.0,1.0,1.0,0.5]|\n",
      "+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rescale each feature to range [min, max].\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "print(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n",
    "scaledData.select(\"features\", \"scaledFeatures\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标准化\n",
    "将属性特征标准化，使得每列的每个元素都有相同的标准差and/or零均值。具体是将数据按属性（按列进行）减去其均值，并除以其方差。得到的结果是，对于每个属性/每列来说所有数据都聚集在0附近，标准差为1。使用z-score方法规范化$(x-mean(x))/std(x)$,spark采用StandardScaler方法，其两个参数withStd: 默认是True.withMean: 默认是False.如果某列特征属性标准偏差是0，则该列元素的值则会返回0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -1.0, 0.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0, 0.0]),),\n",
    "    (2, Vectors.dense([3.0, 10.1, 3.0, 0.0]),)\n",
    "], [\"id\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(dataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------------------------+\n",
      "|id |features          |scaledFeatures                     |\n",
      "+---+------------------+-----------------------------------+\n",
      "|0  |[1.0,0.1,-1.0,0.0]|[-1.0,-0.6657502859356826,-1.0,0.0]|\n",
      "|1  |[2.0,1.1,1.0,0.0] |[0.0,-0.4841820261350419,0.0,0.0]  |\n",
      "|2  |[3.0,10.1,3.0,0.0]|[1.0,1.1499323120707245,1.0,0.0]   |\n",
      "+---+------------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "scaledData.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化\n",
    "主要思想是对每个样本(每行数据)计算其p-范数（这里的p是大于等于1的正整数），然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的p-范数（l1-norm,l2-norm等等）等于1。\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/9f2d83bfa397bdf021046004b9a365079cab6a22)\n",
    "当p趋近于最大值的时候，即![](https://wikimedia.org/api/rest_v1/media/math/render/svg/b0e215b4456f5edc7e31f009440ccc2cc3f61ad4)\n",
    "此时范数的值是该样本所有元素中的最大值![](https://wikimedia.org/api/rest_v1/media/math/render/svg/8c4a4408043912b9b66fd58977a30db576bdf735)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.5, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.0, 1.0]),),\n",
    "    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n",
    "], [\"id\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L^1 norm\n",
      "+---+--------------+------------------+\n",
      "| id|      features|      normFeatures|\n",
      "+---+--------------+------------------+\n",
      "|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n",
      "|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n",
      "|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n",
      "+---+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalize each Vector using $L^1$ norm.\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "l1NormData = normalizer.transform(dataFrame)\n",
    "print(\"Normalized using L^1 norm\")\n",
    "l1NormData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L^inf norm\n",
      "+---+--------------+--------------+\n",
      "| id|      features|  normFeatures|\n",
      "+---+--------------+--------------+\n",
      "|  0|[1.0,0.5,-1.0]|[1.0,0.5,-1.0]|\n",
      "|  1| [2.0,1.0,1.0]| [1.0,0.5,0.5]|\n",
      "|  2|[4.0,10.0,2.0]| [0.4,1.0,0.2]|\n",
      "+---+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalize each Vector using $L^\\infty$ norm.\n",
    "lInfNormData = normalizer.transform(dataFrame, {normalizer.p: float(\"inf\")})\n",
    "print(\"Normalized using L^inf norm\")\n",
    "lInfNormData.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
